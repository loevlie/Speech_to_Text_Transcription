{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "h6Gi1R4nnfxG"
   },
   "source": [
    "# HW4 P2 IDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "id": "OZXkCkQAN4GN"
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "id": "7LPn3mLOoeAe"
   },
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "id": "dyJfQUjIoirw"
   },
   "outputs": [],
   "source": [
    "! cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false",
    "id": "tbOy2oswo3PX"
   },
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39744,
     "status": "ok",
     "timestamp": 1605194565765,
     "user": {
      "displayName": "Denny Loevlie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giscmna1mRfYQrs9gouTR_IIRLYYZJEaX5ksIjcsg=s64",
      "userId": "17735946205138225469"
     },
     "user_tz": 300
    },
    "id": "B7rUpvASo8wo",
    "outputId": "3bbb0106-343c-468a-e7d1-58a37e7f4aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 11-785-fall-20-homework-4-part-2.zip to /home/ubuntu/AWS_HW4_P2\n",
      "100%|█████████████████████████████████████▉| 3.72G/3.72G [00:59<00:00, 34.5MB/s]\n",
      "100%|██████████████████████████████████████| 3.72G/3.72G [00:59<00:00, 67.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c 11-785-fall-20-homework-4-part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  11-785-fall-20-homework-4-part-2.zip\n",
      "  inflating: hw4p2/dev.npy           \n",
      "  inflating: hw4p2/dev_transcripts.npy  \n",
      "  inflating: hw4p2/sample.csv        \n",
      "  inflating: hw4p2/test.npy          \n",
      "  inflating: hw4p2/train.npy         \n",
      "  inflating: hw4p2/train_transcripts.npy  \n"
     ]
    }
   ],
   "source": [
    "!unzip 11-785-fall-20-homework-4-part-2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "id": "kyznzSkUQHz1"
   },
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "id": "dBGmKhnNfucr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "\n",
    "'''\n",
    "Loading all the numpy files containing the utterance information and text information\n",
    "'''\n",
    "def load_data():\n",
    "    speech_train = np.load('./hw4p2/train.npy', allow_pickle=True, encoding='bytes')\n",
    "    speech_valid = np.load('./hw4p2/dev.npy', allow_pickle=True, encoding='bytes')\n",
    "    speech_test = np.load('./hw4p2/test.npy', allow_pickle=True, encoding='bytes')\n",
    "\n",
    "    transcript_train = np.load('./hw4p2/train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
    "    transcript_valid = np.load('./hw4p2/dev_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
    "\n",
    "    return speech_train, speech_valid, speech_test, transcript_train, transcript_valid\n",
    "\n",
    "\n",
    "'''\n",
    "Transforms alphabetical input to numerical input, replace each letter by its corresponding \n",
    "index from letter_list\n",
    "'''\n",
    "\n",
    "def transform_letter_to_index(transcript, letter_list):\n",
    "    '''\n",
    "    :param transcript :(N, ) Transcripts are the text input\n",
    "    :param letter_list: Letter list defined above\n",
    "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
    "    '''\n",
    "    letter_to_index_list = []\n",
    "    for transcript_train in transcript:\n",
    "        char_list = []\n",
    "        for i,word in enumerate(transcript_train):\n",
    "            if i == 0:\n",
    "                char_list.append(letter_list.index('<sos>'))\n",
    "\n",
    "            for char in word.decode('utf-8'):\n",
    "                char_list.append(letter_list.index(char))\n",
    "\n",
    "            if i == len(transcript_train)-1:\n",
    "                char_list.append(letter_list.index('<eos>'))\n",
    "            else:\n",
    "                char_list.append(letter_list.index(' '))\n",
    "        letter_to_index_list.append(char_list)\n",
    "    return letter_to_index_list\n",
    "\n",
    "\n",
    "'''\n",
    "Optional, create dictionaries for letter2index and index2letter transformations\n",
    "'''\n",
    "def create_dictionaries(letter_list):\n",
    "    letter2index = dict()\n",
    "    index2letter = dict()\n",
    "    return letter2index, index2letter\n",
    "\n",
    "\n",
    "class Speech2TextDataset(Dataset):\n",
    "    '''\n",
    "    Dataset class for the speech to text data, this may need some tweaking in the\n",
    "    getitem method as your implementation in the collate function may be different from\n",
    "    ours. \n",
    "    '''\n",
    "    def __init__(self, speech, text=None, isTrain=True):\n",
    "        self.speech = speech\n",
    "        self.isTrain = isTrain\n",
    "        if (text is not None):\n",
    "            self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.speech.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if (self.isTrain == True):\n",
    "            return torch.tensor(self.speech[index].astype(np.float32)),torch.tensor(len(self.speech[index])), torch.tensor(self.text[index]),torch.tensor(len(self.text[index]))\n",
    "        else:\n",
    "            return torch.tensor(self.speech[index].astype(np.float32)),torch.tensor(len(self.speech[index]))\n",
    "\n",
    "\n",
    "def collate_train(seq_list):\n",
    "    ### Return the padded speech and text data, and the length of utterance and transcript ###\n",
    "    X=[]\n",
    "    X_len = []\n",
    "    Y=[]\n",
    "    Y_len = []\n",
    "    for i in range(len(seq_list)):\n",
    "        X.append(seq_list[i][0])\n",
    "        X_len.append(seq_list[i][1])\n",
    "        Y.append(seq_list[i][2])\n",
    "        Y_len.append(seq_list[i][3])\n",
    "        \n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(X,padding_value=0.0)\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(Y,batch_first=True,padding_value=0.0)\n",
    "    return inputs,torch.tensor(X_len),targets,torch.tensor(Y_len)\n",
    "\n",
    "\n",
    "def collate_test(seq_list):\n",
    "    ### Return padded speech and length of utterance ###\n",
    "    X=[]\n",
    "    X_len = []\n",
    "    for i in range(len(seq_list)):\n",
    "        X.append(seq_list[i][0])\n",
    "        X_len.append(seq_list[i][1])\n",
    "        \n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(X,padding_value=0.0)\n",
    "    return inputs,torch.tensor(X_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "speech_train, speech_valid, speech_test, transcript_train, transcript_valid = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_attn_flow(attn_mask, path):\n",
    "    plt.imsave(path, attn_mask, cmap='hot')\n",
    "    return plt\n",
    "\n",
    "def plot_grad_flow(named_parameters, path):\n",
    "    ave_grads = []\n",
    "    max_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            if(p is not None):\n",
    "                layers.append(n)\n",
    "                ave_grads.append(p.grad.abs().mean())\n",
    "                max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    #plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
    "    plt.show()\n",
    "    plt.savefig(path)\n",
    "    return plt, max_grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention is calculated using key, value and query from Encoder and decoder.\n",
    "    Below are the set of operations you need to perform for computing attention:\n",
    "        energy = bmm(key, query)\n",
    "        attention = softmax(energy)\n",
    "        context = bmm(attention, value)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value, lens):\n",
    "        '''query, context, lengths):\n",
    "        :param query :(batch_size, hidden_size) Query is the output of LSTMCell from Decoder\n",
    "        :param keys: (batch_size, max_len, encoder_size) Key Projection from Encoder\n",
    "        :param values: (batch_size, max_len, encoder_size) Value Projection from Encoder\n",
    "        :return context: (batch_size, encoder_size) Attended Context\n",
    "        :return attention_mask: (batch_size, max_len) Attention mask that can be plotted \n",
    "        '''\n",
    "                \n",
    "        key = key.permute(1,0,2)\n",
    "        value = value.permute(1,0,2)\n",
    "\n",
    "        attention = torch.bmm(key, query.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        mask = torch.arange(key.size(1)).unsqueeze(0) >= lens.unsqueeze(1)\n",
    "        \n",
    "        # Set attention logits at padding positions to negative infinity.\n",
    "        attention.masked_fill_(mask.cuda(), -1e9)\n",
    "        \n",
    "        # Take softmax over the \"source length\" dimension.\n",
    "        attention = nn.functional.softmax(attention, dim=1)\n",
    "        \n",
    "        out = torch.bmm(attention.unsqueeze(1), value).squeeze(1)\n",
    "        \n",
    "        # attention vectors are returned for visualization\n",
    "        return out, attention\n",
    "\n",
    "class pBLSTM(nn.Module):\n",
    "    '''\n",
    "    Pyramidal BiLSTM\n",
    "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
    "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
    "    and inferior results even after extensive training.\n",
    "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
    "    from a large number of input steps.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x :(N, T) input to the pBLSTM\n",
    "        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM \n",
    "        '''\n",
    "        \n",
    "        x,lens = unpack(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        if x.shape[1] % 2 != 0:\n",
    "            x = x[:,:-1,:]\n",
    "        x = x.reshape(x.shape[0],int(x.shape[1]/2),x.shape[2]*2)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = pack(x,lengths=lens/2,enforce_sorted=False)\n",
    "        x,_ = self.blstm(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder takes the utterances as inputs and returns the key and value.\n",
    "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        ### Add code to define the blocks of pBLSTMs! ###\n",
    "        self.pblstm1 = pBLSTM(hidden_dim*4,hidden_dim)\n",
    "        self.pblstm2 = pBLSTM(hidden_dim*4,hidden_dim)\n",
    "        self.pblstm3 = pBLSTM(hidden_dim*4,hidden_dim)\n",
    "        self.pblstm4 = pBLSTM(hidden_dim*4,hidden_dim)\n",
    "        \n",
    "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
    "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
    "        outputs, _ = self.lstm(rnn_inp)\n",
    "        outputs = self.pblstm1(outputs)\n",
    "        outputs = self.pblstm2(outputs)\n",
    "        outputs = self.pblstm3(outputs)\n",
    "        outputs = self.pblstm4(outputs)\n",
    "        ### Use the outputs and pass it through the pBLSTM blocks! ###\n",
    "        \n",
    "        linear_input, lengths = utils.rnn.pad_packed_sequence(outputs)\n",
    "        keys = self.key_network(linear_input)\n",
    "        value = self.value_network(linear_input)\n",
    "        return keys, value, lengths \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
    "    thus we use LSTMCell instead of LSLTM here.\n",
    "    The output from the second LSTMCell can be used as query here for attention module.\n",
    "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
    "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
    "    '''\n",
    "    def __init__(self, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
    "\n",
    "        self.isAttended = isAttended\n",
    "        if (isAttended == True):\n",
    "            self.attention = Attention()\n",
    "\n",
    "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
    "\n",
    "    def forward(self, key, values, text=None, isTrain=True,lens=None):\n",
    "        '''\n",
    "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
    "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
    "        :param text: (N, text_len) Batch input of text with text_length\n",
    "        :param isTrain: Train or eval mode\n",
    "        :return predictions: Returns the character perdiction probability \n",
    "        '''\n",
    "        batch_size = key.shape[1]\n",
    "\n",
    "        if (isTrain == True):\n",
    "            #print(text)\n",
    "            max_len =  text.shape[1]\n",
    "            embeddings = self.embedding(text)\n",
    "        else:\n",
    "            max_len = 250\n",
    "\n",
    "        predictions = []\n",
    "        hidden_states = [None, None]\n",
    "        prediction = torch.zeros(batch_size,1).to(DEVICE)#(torch.ones(batch_size, 1)*33).to(DEVICE)\n",
    "        context = values[0,:,:]\n",
    "        attention_list = []\n",
    "        for i in range(max_len):\n",
    "            # * Implement Gumble noise and teacher forcing techniques \n",
    "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
    "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
    "            #   out of the loop so you do not get index out of range errors. \n",
    "            \n",
    "            if (isTrain):\n",
    "                char_embed = embeddings[:,i,:]\n",
    "            else:\n",
    "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "            \n",
    "            inp = torch.cat([char_embed,context], dim=1)\n",
    "            \n",
    "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
    "\n",
    "            inp_2 = hidden_states[0][0]\n",
    "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
    "\n",
    "            ### Compute attention from the output of the second LSTM Cell ###\n",
    "            output = hidden_states[1][0]\n",
    "            \n",
    "            if self.isAttended==True:\n",
    "                context, attention = self.attention(output,key,values,lens)\n",
    "                attention_list.append(attention[0].detach().cpu().numpy())\n",
    "                prediction = self.character_prob(torch.cat([output, context], dim=1))\n",
    "                predictions.append(prediction.unsqueeze(1))\n",
    "            else:\n",
    "                prediction = self.character_prob(torch.cat([output, values[i,:,:]], dim=1))\n",
    "                predictions.append(prediction.unsqueeze(1))\n",
    "        if self.isAttended==True:\n",
    "            return torch.cat(predictions, dim=1),attention_list\n",
    "        else:\n",
    "            return torch.cat(predictions, dim=1),None\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    '''\n",
    "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
    "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim)\n",
    "        self.decoder = Decoder(vocab_size, hidden_dim,isAttended=True)\n",
    "\n",
    "    def forward(self, speech_input, speech_len, text_input=None, isTrain=True):\n",
    "        key, value,lengths = self.encoder(speech_input, speech_len)\n",
    "        if (isTrain == True):\n",
    "            predictions,attention = self.decoder(key, value, text_input,lens=lengths)\n",
    "        else:\n",
    "            predictions,attention = self.decoder(key, value, text=None, isTrain=False,lens=lengths)\n",
    "        return predictions,attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "### Add Your Other Necessary Imports Here! ###\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    model.to(DEVICE)\n",
    "    loss_history = []\n",
    "    start = time.time()\n",
    "    all_attentions = []\n",
    "    for j, (X,X_lens,Y,Y_lens) in enumerate(train_loader):\n",
    "        X,Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "        X_lens,Y_lens= X_lens.to(DEVICE),Y_lens.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "        out,attention_list = model(X,X_lens,Y[:,:-1])\n",
    "        loss += criterion(out.reshape(out.shape[0]*out.shape[1],out.shape[-1]), Y[:,1:].reshape(Y[:,1:].shape[0]*Y[:,1:].shape[1]))\n",
    "        \n",
    "        if j % 15 == 0:\n",
    "            print(f'Batch {j} has loss: {loss.item()}')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        del X\n",
    "        del X_lens\n",
    "        del Y\n",
    "        del Y_lens\n",
    "        del loss\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Time: {(end-start)/60}')\n",
    "    return attention_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "LETTER_LIST = ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
    "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']\n",
    "\n",
    "\n",
    "model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=128)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean',ignore_index=0)\n",
    "nepochs = 25\n",
    "batch_size = 64 if DEVICE == 'cuda' else 1 \n",
    "character_text_train = transform_letter_to_index(transcript_train, LETTER_LIST)\n",
    "character_text_valid = transform_letter_to_index(transcript_valid, LETTER_LIST)\n",
    "\n",
    "train_dataset = Speech2TextDataset(speech_train, character_text_train)\n",
    "\n",
    "test_dataset = Speech2TextDataset(speech_test, None, False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_test)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    attn_mask = train(model, train_loader, criterion, optimizer, epoch)\n",
    "    plot_attn_flow(attn_mask, f'Epoch_{epoch}.png')\n",
    "    test(model, test_loader, epoch)\n",
    "    print(f'EPOCH {epoch}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training for a few more epochs to see if it starts \"Paying Attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    attn_mask = train(model, train_loader, criterion, optimizer, epoch)\n",
    "    plot_attn_flow(attn_mask, f'Epoch_{10+epoch}.png')\n",
    "    # val()\n",
    "    test(model, test_loader, epoch)\n",
    "    print(f'EPOCH {epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "Final = []\n",
    "for i, (X,X_len) in enumerate(test_loader):\n",
    "    X,X_len = X.to(DEVICE),X_len.to(DEVICE)\n",
    "    out,_ = model(X,X_len,isTrain=False)\n",
    "    _,out2 = torch.max(out,dim=2)\n",
    "    out2 = out2.detach().cpu().numpy()\n",
    "    for batch in out2:\n",
    "        out = []\n",
    "        for word in batch:\n",
    "            if word == 33: # 33 --> <sos>\n",
    "                None\n",
    "            elif word == 34: # 34 --> <eos>\n",
    "                break\n",
    "            else:\n",
    "                out.append(LETTER_LIST[word])\n",
    "        Final.append(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 652k/652k [00:03<00:00, 206kB/s]\n",
      "Successfully submitted to 11-785-Fall-20-Homework 4 Part 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ID = [i for i in range(len(Final))]\n",
    "df_pred = pd.DataFrame(data={'Id':ID,'label':Final})\n",
    "df_pred.to_csv(f'Prediction2.csv',index=False)\n",
    "!kaggle competitions submit -c 11-785-fall-20-homework-4-part-2 -f Prediction2.csv -m \"Submission last\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OZXkCkQAN4GN",
    "kyznzSkUQHz1",
    "LxrMiMpxHhFY"
   ],
   "machine_shape": "hm",
   "name": "HW3_P2_Transcript_Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
